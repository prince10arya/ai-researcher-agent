\documentclass{article}\n\\usepackage{amsmath}\n\\usepackage{amsfonts}\n\\usepackage{amssymb}\n\\usepackage{graphicx}\n\\usepackage[utf8]{inputenc}\n\\usepackage{hyperref}\n\n\\title{Scaffolded Group Relative Policy Optimization (Scaf-GRPO): A Framework for Robust LLM Reasoning Enhancement}\n\\author{AI Research Assistant}\n\\date{}\n\n\\begin{document}\n\n\\maketitle\n\n\\begin{abstract}\nLarge Language Models (LLMs) have shown remarkable capabilities in complex reasoning tasks, often enhanced through Reinforcement Learning from Verifiable Rewards (RLVR). However, these methods are frequently hampered by the "learning cliff" phenomenon, where models fail on problems significantly beyond their current capabilities, leading to persistent zero-reward signals and stalled learning. This paper introduces Scaffolded Group Relative Policy Optimization (Scaf-GRPO), a novel progressive training framework designed to overcome this challenge. Inspired by pedagogical scaffolding, Scaf-GRPO strategically provides hierarchical, minimal in-prompt guidance only when a model\'s independent learning plateaus. The framework employs a two-phase approach: an initial guidance exemption period to diagnose "true-hard" problems, followed by hierarchical hint-guided exploration. By augmenting the training batch with minimally guided successful trajectories, Scaf-GRPO restores the learning gradient, enabling models to learn from previously intractable problems. Experimental results on challenging mathematics benchmarks demonstrate Scaf-GRPO\'s significant superiority over vanilla GRPO and existing prefix-based guidance methods, fostering genuine skill acquisition and improved generalization. This work provides a robust methodology for extending the frontier of autonomous reasoning in LLMs.\n\\end{abstract}\n\n\\section{Introduction}\nThe advent of Large Language Models (LLMs) has revolutionized capabilities across diverse domains, from natural language understanding to complex reasoning tasks in mathematics, programming, and logic \\cite{arxiv.org/pdf/2501.12948v1}. A pivotal technique for enhancing these complex reasoning abilities is Reinforcement Learning from Verifiable Rewards (RLVR), where models learn by exploring various strategies and receiving feedback on their final outcomes, circumventing the need for expensive, step-by-step human annotations \\cite{arxiv.org/pdf/2501.12948v1}. This paradigm allows models to autonomously discover effective problem-solving procedures.\n\nDespite its promise, RLVR is fundamentally constrained by a phenomenon we term the "learning cliff." This occurs when an LLM encounters problems that are far beyond its current capabilities. In such scenarios, all exploratory attempts consistently fail, leading to a persistent zero-reward signal. For policy optimization algorithms like Group Relative Policy Optimization (GRPO) \\cite{arxiv.org/pdf/2403.14476v1}, this collapses the advantage calculation to zero, effectively rendering these difficult problems "invisible" to the learning gradient and stalling progress. Consequently, these problems form a persistent "long tail" of challenges that the model cannot conquer autonomously, preventing it from leveraging the most difficult examples to achieve a higher level of competence.\n\nTo address this critical bottleneck, existing strategies often incorporate off-policy guidance from a more capable "teacher" policy, typically by providing a prefix of a correct "golden" solution \\cite{arxiv.org/pdf/2504.14945v1}. While this ensures a positive reward signal, it introduces significant issues such as distributional mismatches between teacher-generated prefixes and student-generated suffixes, necessitating complex algorithmic corrections. More importantly, this "on-rails" guidance stifles the model\'s ability to explore alternative, potentially more novel or efficient, reasoning strategies.\n\nThis paper introduces \\textbf{Scaf-GRPO (Scaffolded Group Relative Policy Optimization)}, a novel progressive training framework inspired by pedagogical scaffolding \\cite{scaffolding}. Scaf-GRPO provides hierarchical, minimal, and progressive assistance to help LLMs bridge their capability gaps without enforcing rigid solution prefixes. Our in-prompt scaffolding approach is guided by two primary objectives: first, to maintain policy consistency by having the model process both the problem and the hint under a single, unified policy, thereby avoiding the distributional mismatches of prefix-based methods. Second, to preserve exploration flexibility, as our hints act as "signposts" rather than "railroads," guiding the model without fixing its path and allowing it to discover its own unique solution strategies.\n\n\\section{Background: GRPO and the Learning Cliff}\nReinforcement Learning from Verifier Reward (RLVR) has become a cornerstone for enhancing LLM reasoning. In this paradigm, models generate solutions, and an external verifier provides an outcome-based reward (e.g., correct/incorrect). DeepSeek-R1 \\cite{arxiv.org/pdf/2501.12948v1} demonstrated that even with sparse, binary rewards, models can learn complex reasoning strategies.\n\nGroup Relative Policy Optimization (GRPO) \\cite{arxiv.org/pdf/2403.14476v1} is an on-policy RL algorithm used for training LLMs that eliminates the need for a trainable value function. For a given prompt $q$, the policy $\\pi_\\theta$ generates a group of $N$ trajectories, $G = \\{o_1, \\dots, o_N\\}$. After obtaining a terminal reward $R(o_i)$ for each trajectory from a verifier, GRPO computes a normalized advantage $\\hat{A}_i$ as:\n\\begin{equation}\n    \\hat{A}_i = \\frac{R(o_i) - \\mu_G}{\\sigma_G + \\epsilon_{std}}\n\\end{equation}\nwhere $\\mu_G$ and $\\sigma_G$ are the mean and standard deviation of rewards in the group $G$, and $\\epsilon_{std}$ is a small constant for numerical stability. The policy is then updated by maximizing a clipped surrogate objective:\n\\begin{equation}\n    J_{GRPO}(\\theta) = \\hat{\\mathbb{E}}_{i,t} \\left[ \\min \\left( r_{i,t}(\\theta) \\hat{A}_i, \\text{clip}(r_{i,t}(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_i \\right) \\right]\n\\end{equation}\nwhere $r_{i,t}(\\theta) = \\frac{\\pi_\\theta(o_{i,t}|o_{i,<t},q)}{\\pi_{\\theta_{old}}(o_{i,t}|o_{i,<t},q)}$ is the probability ratio between the current and old policies, and $\\epsilon$ is the clipping hyperparameter.\n\nThe key limitation, the \\textit{learning cliff}, arises when all trajectories in $G$ receive a zero reward. In this scenario, $\\mu_G$ and $\\sigma_G$ become zero, causing $\\hat{A}_i$ to collapse to zero for the entire group and stalling the learning process. This lack of a learning signal for difficult problems prevents models from improving on the most challenging examples.\n\n\\section{The Scaf-GRPO Framework}\nScaf-GRPO modifies the GRPO training process by strategically augmenting the trajectory group $G$ when a learning cliff is detected. The framework operates in two carefully designed phases: an initial guidance exemption period and a subsequent cyclical phase of hierarchical hint-guided exploration.\n\n\\subsection{Phase 1: Diagnosing True-Hard Problems (Guidance Exemption)}\nA crucial principle in effective teaching is to avoid providing unnecessary help. Not all initial failures indicate a fundamental capability gap; many are "pseudo-hard" samples stemming from issues like unfamiliarity with output formats or nascent reasoning skills. Scaf-GRPO incorporates a guidance exemption period, typically the initial 15\\% of training steps. During this phase, the model attempts solutions purely through on-policy exploration. This period allows the model to overcome basic execution issues independently. Guidance is only activated when the rate of solving zero-reward queries stagnates, identifying problems as "true-hard" and thus candidates for intervention.\n\n\\subsection{Phase 2: Hierarchical Hint-Guided Exploration}\nOnce a problem is identified as "true-hard," Scaf-GRPO activates its guidance mechanism using a pre-defined, three-tiered hint hierarchy, $H = \\{H_{knowledge}, H_{planning}, H_{solution}\\}$. These tiers offer distinct levels of guidance:\n\\begin{enumerate}\n    \\item \\textbf{$H_{knowledge}$ (Knowledge Hint):} Points to the key concept or formula required.\n    \\item \\textbf{$H_{planning}$ (Planning Hint):} Outlines a high-level strategic framework for the solution.\n    \\item \\textbf{$H_{solution}$ (Solution Hint):} Provides a concrete calculation step.\n\\end{enumerate}\nTo provide the minimal necessary guidance, the framework executes a deterministic search through this hierarchy, proceeding from the most abstract to the most concrete hint ($H_{knowledge} \\rightarrow H_{planning} \\rightarrow H_{solution}$). Within each tier, guidance is offered incrementally. The search terminates as soon as the model generates a correct solution, thereby identifying the minimal effective guidance required. This approach encourages the internalization of reasoning skills rather than the memorization of solutions.\n\n\\subsection{On-Policy Batch Augmentation and Unified Loss}\nThe core of Scaf-GRPO is its on-policy intervention, designed to reactivate the learning signal during a learning cliff. When all initial trajectories $G=\\{o_1, \\dots, o_N\\}$ from $\\pi_\\theta(\\cdot|q)$ yield zero reward, the advantage $\\hat{A}_i$ collapses, halting the gradient update. Scaf-GRPO intervenes by finding a minimal hint $h^*$ that enables policy $\\pi_\\theta$ to generate a successful trajectory $o^*_h \\sim \\pi_\\theta(\\cdot|q \\oplus h^*)$, where $\\oplus$ denotes the concatenation of the hint into the prompt. This successful trajectory replaces a random failed trajectory $o_j \\in G$ to form an augmented group, $G_{final} = (G \\setminus \\{o_j\\}) \\cup \\{o^*_h\\}$.\n\nCrucially, Scaf-GRPO does not alter the mathematical form of the GRPO loss function. Instead, it modifies the data used for the loss computation. The advantage calculation is performed on this conditionally augmented batch:\n\\begin{equation}\n    \\hat{A}\'_i = \\frac{R(o\'_i) - \\mu_{G_{final}}}{\\sigma_{G_{final}} + \\epsilon_{std}} \\quad \\text{for } o\'_i \\in G_{final}\n\\end{equation}\nThe learning objective remains the clipped surrogate objective, but it is now applied to the trajectories in $G_{final}$. The probability ratio for a given trajectory $o\'_i \\in G_{final}$ at timestep $t$ is critically computed with respect to the trajectory\'s specific originating prompt:\n\\begin{equation}\n    r\'_{i,t}(\\theta) = \\begin{cases}\n        \\frac{\\pi_\\theta(o\'_{i,t}|o\'_{i,<t},q)}{\\pi_{\\theta_{old}}(o\'_{i,t}|o\'_{i,<t},q)} & \\text{if } o\'_i \\in G_{final} \\text{ and } o\'_i \\neq o^*_h \\\\\n        \\frac{\\pi_\\theta(o\'_{i,t}|o\'_{i,<t},q \\oplus h^*)}{\\pi_{\\theta_{old}}(o\'_{i,t}|o\'_{i,<t},q \\oplus h^*)} & \\text{if } o\'_i = o^*_h\n    \\end{cases}\n\\end{equation}\nThis on-policy augmentation ensures the batch contains non-zero reward variance, restoring a meaningful advantage signal and allowing learning to resume on previously intractable problems. This preserves the on-policy principle, avoiding the high variance and instability associated with off-policy corrections.\n\n\\section{Experimental Setup and Results}\nThe effectiveness of Scaf-GRPO was demonstrated through extensive experiments on several challenging mathematics benchmarks, including AIME24, AIME25, AMC, Minerva, MATH-500, Olympiad, and GaoKao2023en. Experiments were conducted on diverse LLM architectures, such as the Qwen2.5 series, Llama-3.2-3B-Instruct, and the Long Chain-of-Thought model DeepSeek-R1-Distill-Qwen-1.5B.\n\n\\subsection{Key Findings}\n\\begin{itemize}\n    \\item \\textbf{Significant Performance Gains:} Scaf-GRPO consistently achieved substantial performance improvements across all tested models and benchmarks. For instance, on the AIME24 benchmark, the Qwen2.5-Math-7B model saw a relative `pass@1` score boost of 44.3\\% over the vanilla GRPO baseline.\n    \\item \\textbf{Outperformance of Baselines:} Scaf-GRPO demonstrated clear superiority over vanilla GRPO and other leading methods, including prefix-based guidance approaches like LUFFY \\cite{arxiv.org/pdf/2504.14945v1}. This highlights the efficacy of the in-prompt scaffolding strategy compared to altering trajectories.\n    \\item \\textbf{Generalization Across Models and Tasks:} The framework proved to be model-agnostic, showing consistent gains on different architectures (Qwen, Llama) and specializations (math-tuned, instruction-tuned, LongCoT models). Furthermore, Scaf-GRPO fostered robust reasoning skills that generalized to out-of-distribution tasks, such as the expert-level scientific questions in the GPQA-Diamond benchmark \\cite{arxiv.org/pdf/2402.14008v1}.\n    \\item \\textbf{Ablation Studies Validation:} Ablation studies confirmed the critical role of each component: the guidance exemption period prevents over-reliance on hints, and the progressive, hierarchical nature of hints fosters more generalizable reasoning skills over direct solution provision.\n\\end{itemize}\n\n\\section{Discussion and Future Work}\nScaf-GRPO presents a significant step forward in enhancing LLM reasoning by effectively addressing the learning cliff. Its pedagogical approach, which provides minimal, hierarchical guidance only when needed, allows models to internalize reasoning skills rather than merely imitating solutions. This preserves the exploratory autonomy of LLMs and avoids the distributional consistency issues inherent in prefix-continuation methods.\n\n\\subsection{Limitations}\nThe practical deployment of Scaf-GRPO currently relies on the availability of a high-quality, tiered hint hierarchy. Generating these structured hints requires a non-trivial data preparation effort. Additionally, the framework is primarily designed for tasks with verifiable solutions and structured reasoning paths, such as mathematics. Its applicability to more open-ended, subjective domains like creative writing is less direct.\n\n\\subsection{Future Work}\nFuture research should focus on several promising directions:\n\\begin{enumerate}\n    \\item \\textbf{Automated Hint Generation:} Developing methods to automatically generate high-quality, tiered hint hierarchies would significantly enhance the framework\'s scalability and reduce manual data preparation. This could involve using more powerful LLMs to self-generate hints or leveraging symbolic reasoning systems.\n    \\item \\textbf{Adaptive Scaffolding Mechanisms:} Exploring adaptive guidance mechanisms where the level and type of assistance dynamically adjust to the model\'s improving proficiency. This personalized learning process could optimize the balance between guidance and autonomous exploration.\n    \\item \\textbf{Extending to Other Domains:} Investigating the applicability of Scaf-GRPO to other complex reasoning domains, such as scientific discovery, logical inference, or code generation, where structured problem-solving can benefit from targeted guidance.\n    \\item \\textbf{Theoretical Analysis of Convergence:} A deeper theoretical analysis of Scaf-GRPO\'s convergence properties and its robustness against various types of learning cliffs would provide further insights into its mechanisms.\n\\end{enumerate}\n\n\\section{Conclusion}\nIn this work, we introduced Scaf-GRPO, a novel training framework that effectively overcomes the "learning cliff" phenomenon in reinforcement learning for large language models. By providing hierarchical, minimal in-prompt guidance, Scaf-GRPO enables models to solve problems previously beyond their reach. This on-policy guidance preserves exploratory autonomy and mitigates the distributional consistency issues inherent in prefix-continuation methods. Our extensive experiments demonstrate that Scaf-GRPO significantly outperforms vanilla GRPO and strong prefix-based baselines across challenging mathematics benchmarks, establishing a more effective path toward robust and autonomous reasoning in LLMs.\n\n\\begin{thebibliography}{99}\n\n\\bibitem{arxiv.org/pdf/2510.19807v1}\nXichen Zhang, Sitong Wu, Yinghao Zhu, Haoru Tan, Shaozuo Yu, Ziyi He, Jiaya Jia.\n\\newblock \\href{http://arxiv.org/pdf/2510.19807v1}{\\textit{Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning}}.\n\\newblock arXiv preprint arXiv:2510.19807, 2025.\n\n\\bibitem{arxiv.org/pdf/2501.12948v1}\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.\n\\newblock \\href{http://arxiv.org/pdf/2501.12948v1}{\\textit{DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning}}.\n\\newblock arXiv preprint arXiv:2501.12948, 2025.\n\n\\bibitem{arxiv.org/pdf/2403.14476v1}\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian Fan, Gaohong Liu, Lingjun Liu, et al.\n\\newblock \\href{http://arxiv.org/pdf/2403.14476v1}{\\textit{DAPO: An open-source LLM reinforcement learning system at scale}}.\n\\newblock arXiv preprint arXiv:2503.14476, 2025.\n\n\\bibitem{arxiv.org/pdf/2504.14945v1}\nJianhao Yan, Yafu Li, Zican Hu, Zhi Wang, Ganqu Cui, Xiaoye Qu, Yu Cheng, and Yue Zhang.\n\\newblock \\href{http://arxiv.org/pdf/2504.14945v1}{\\textit{Learning to reason under off-policy guidance}}.\n\\newblock arXiv preprint arXiv:2504.14945, 2025.\n\n\\bibitem{scaffolding}\nLaura E. Berk and Adam Winsler.\n\\newblock \\textit{Scaffolding Children’s Learning: Vygotsky and Early Childhood Education}.\n\\newblock National Association for the Education of Young Children, Washington, DC, 1995.\n\n\\bibitem{arxiv.org/pdf/2402.14008v1}\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al.\n\\newblock \\href{http://arxiv.org/pdf/2402.14008v1}{\\textit{OlympiadBench: A challenging benchmark for promoting AGI with Olympiad-level bilingual multimodal scientific problems}}.\n\\newblock arXiv preprint arXiv:2402.14008, 2024.\n\n\\end{thebibliography}\n\n\\end{document}\n